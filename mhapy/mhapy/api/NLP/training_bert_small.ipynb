{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict, Counter\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train on gpu: True\n",
      "1 gpus detected.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    dev = \"cuda\" \n",
    "else: \n",
    "    dev = \"cpu\" \n",
    "device = torch.device(dev) \n",
    "print(device)\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "print(f'Train on gpu: {train_on_gpu}')\n",
    "\n",
    "# Number of gpus\n",
    "if train_on_gpu:\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f'{gpu_count} gpus detected.')\n",
    "    if gpu_count > 1:\n",
    "        multi_gpu = True\n",
    "    else:\n",
    "        multi_gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Datasets/Mental Health Dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                posts predicted  intensity\n",
      "4   gmos now link to leukemia http nsnbc I 2013 07...   neutral          0\n",
      "5   here is a link for an interesting article and ...   neutral          0\n",
      "8   the third know human retrovirus xmrv seem to b...   neutral          0\n",
      "9   leukemia survivor meet his bone marrow donor w...   neutral          0\n",
      "14  new video from patient power study bring posit...   neutral          0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6332 entries, 4 to 10389\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   posts      6332 non-null   object\n",
      " 1   predicted  6332 non-null   object\n",
      " 2   intensity  6332 non-null   int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 197.9+ KB\n",
      "None\n",
      "intensity\n",
      " 0    2921\n",
      "-1    2307\n",
      "-2     673\n",
      " 1     431\n",
      "Name: count, dtype: int64\n",
      "intensity\n",
      " 0    2921\n",
      "-1    2307\n",
      "-2    1346\n",
      " 1     862\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "long_posts_df = df[df['posts'].str.split().str.len() > 200]\n",
    "\n",
    "#eject all posts with more than 200 words\n",
    "short_posts_df = df[df['posts'].str.split().str.len() <= 200]\n",
    "print(short_posts_df.head())\n",
    "print(short_posts_df.info())\n",
    "\n",
    "print(short_posts_df['intensity'].value_counts())\n",
    "\n",
    "\n",
    "duplicate = short_posts_df[(short_posts_df['intensity'] == 1) | (short_posts_df['intensity'] == -2)]\n",
    "short_posts_df = pd.concat([short_posts_df, duplicate])\n",
    "\n",
    "print(short_posts_df['intensity'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                posts predicted\n",
      "4   gmos now link to leukemia http nsnbc I 2013 07...   neutral\n",
      "5   here is a link for an interesting article and ...   neutral\n",
      "8   the third know human retrovirus xmrv seem to b...   neutral\n",
      "9   leukemia survivor meet his bone marrow donor w...   neutral\n",
      "14  new video from patient power study bring posit...   neutral\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7436 entries, 4 to 10388\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   posts      7436 non-null   object\n",
      " 1   predicted  7436 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 174.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_df = short_posts_df.drop('intensity', axis=1)\n",
    "\n",
    "print(new_df.head())\n",
    "print(new_df.info())\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('prajjwal1/bert-mini', do_lower_case=True)\n",
    "\n",
    "tokenized = tokenizer.batch_encode_plus(\n",
    "    new_df['posts'].tolist(),\n",
    "    add_special_tokens=False,\n",
    "    max_length=200,\n",
    "    padding='longest',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    return_attention_mask=False,\n",
    "    return_token_type_ids=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids'])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7436, 200])\n",
      "torch.Size([7436])\n",
      "tensor([2, 2, 2,  ..., 3, 3, 0], device='cuda:0')\n",
      "Class 2: 2921 occurrences\n",
      "Class 1: 2307 occurrences\n",
      "Class 3: 862 occurrences\n",
      "Class 0: 1346 occurrences\n",
      "torch.int64\n",
      "torch.Size([7436, 200])\n",
      "torch.Size([7436])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "tokenized['intensity'] = le.fit_transform(short_posts_df['intensity'])\n",
    "tokenized['intensity'] = torch.tensor(tokenized['intensity'])\n",
    "\n",
    "\n",
    "tokenized['input_ids'] = tokenized['input_ids'].to(device)\n",
    "tokenized['intensity'] = tokenized['intensity'].to(device)\n",
    "print(tokenized['input_ids'].shape) \n",
    "print(tokenized['intensity'].shape)\n",
    "\n",
    "print(tokenized['intensity'])\n",
    "counter = Counter(tokenized['intensity'].tolist())\n",
    "for label, count in counter.items():\n",
    "    print(f\"Class {label}: {count} occurrences\")\n",
    "    \n",
    "print(tokenized['intensity'].dtype)\n",
    "\n",
    "\n",
    "\n",
    "total_dataset = TensorDataset(tokenized['input_ids'], tokenized['intensity'])\n",
    "print(total_dataset.tensors[0].shape)\n",
    "print(total_dataset.tensors[1].shape)\n",
    "\n",
    "\n",
    "train_size = int(0.7 * len(total_dataset))\n",
    "val_size = len(total_dataset) - train_size\n",
    "test_size = int(0.5 * val_size)\n",
    "val_size = val_size - test_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(total_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Class 0 is most negative, class 1 is negative, class 2 is neutral, class 3 is positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[13938,  2891,  2085,  ...,     0,     0,     0],\n",
      "        [ 2182,  2003,  1037,  ...,     0,     0,     0],\n",
      "        [ 1996,  2353,  2113,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 2204,  2851,  2035,  ...,     0,     0,     0],\n",
      "        [ 1037,  2095,  3283,  ...,     0,     0,     0],\n",
      "        [ 2026,  2905,  1999,  ...,     0,     0,     0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(total_dataset.tensors[0][0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperpameters = {'batch_size': 150, 'learning_rate': 0.001, 'epochs': 50, 'optimizer': 'Adam'} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benjamin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-mini and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = hyperpameters['batch_size'],  \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=hyperpameters['batch_size'],  \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=hyperpameters['batch_size'],  \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "                                      'prajjwal1/bert-mini', \n",
    "                                      num_labels = 4,\n",
    "                                      output_attentions = False,\n",
    "                                      output_hidden_states = False\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 256, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 256)\n",
      "      (token_type_embeddings): Embedding(2, 256)\n",
      "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-3): 4 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=4, bias=True)\n",
      "    (1): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "cuda:0\n",
      "<class 'transformers.models.bert.modeling_bert.BertForSequenceClassification'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(256, 4),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "model.classifier = classifier\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.cuda()\n",
    "print(model)\n",
    "print(model.device)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamW = torch.optim.AdamW(model.parameters())\n",
    "hyperpameters['optimizer'] = adamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, test_dataloader, epochs, optimizer, criterion, train_on_gpu, save_path, save=False):\n",
    "    history = []\n",
    "    valid_loss_min = np.Inf\n",
    "    overall_start = timer()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = timer()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        valid_acc = 0\n",
    "        train_acc = 0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for index, (data, target) in enumerate(train_dataloader):\n",
    "            \n",
    "            if train_on_gpu:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data).logits\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "            \n",
    "            print(f'Epoch: {epoch}\\t{100 * (index + 1) / len(train_dataloader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.', end='\\r')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for data, target in val_dataloader:\n",
    "                    # Tensors to gpu\n",
    "                if train_on_gpu:\n",
    "                    data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                    \n",
    "                output = model(data).logits\n",
    "\n",
    "                    \n",
    "                loss = criterion(output, target)\n",
    "                    \n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "                accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "                valid_acc += accuracy.item() * data.size(0)\n",
    "            \n",
    "            train_loss = train_loss / len(train_dataloader.dataset)\n",
    "            valid_loss = valid_loss / len(val_dataloader.dataset)\n",
    "\n",
    "            # Calculate average accuracy\n",
    "            train_acc = train_acc / len(train_dataloader.dataset)\n",
    "            valid_acc = valid_acc / len(val_dataloader.dataset)\n",
    "            history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
    "            \n",
    "            print(f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}')\n",
    "            print(f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%')\n",
    "    model.optimizer = optimizer\n",
    "    total_time = timer() - overall_start\n",
    "    print(f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.')\n",
    "    history = pd.DataFrame(\n",
    "        history,\n",
    "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t100.00% complete. 3.21 seconds elapsed in epoch.\n",
      "Epoch: 0 \tTraining Loss: 1.3094 \tValidation Loss: 1.2799\n",
      "\t\tTraining Accuracy: 40.29%\t Validation Accuracy: 44.27%\n",
      "Epoch: 1\t100.00% complete. 2.71 seconds elapsed in epoch.\n",
      "Epoch: 1 \tTraining Loss: 1.2661 \tValidation Loss: 1.2648\n",
      "\t\tTraining Accuracy: 45.63%\t Validation Accuracy: 46.95%\n",
      "Epoch: 2\t100.00% complete. 2.73 seconds elapsed in epoch.\n",
      "Epoch: 2 \tTraining Loss: 1.2548 \tValidation Loss: 1.2539\n",
      "\t\tTraining Accuracy: 47.03%\t Validation Accuracy: 45.88%\n",
      "Epoch: 3\t100.00% complete. 2.64 seconds elapsed in epoch.\n",
      "Epoch: 3 \tTraining Loss: 1.2484 \tValidation Loss: 1.2474\n",
      "\t\tTraining Accuracy: 48.24%\t Validation Accuracy: 47.94%\n",
      "Epoch: 4\t100.00% complete. 2.66 seconds elapsed in epoch.\n",
      "Epoch: 4 \tTraining Loss: 1.2446 \tValidation Loss: 1.2435\n",
      "\t\tTraining Accuracy: 49.13%\t Validation Accuracy: 48.03%\n",
      "Epoch: 5\t100.00% complete. 2.63 seconds elapsed in epoch.\n",
      "Epoch: 5 \tTraining Loss: 1.2407 \tValidation Loss: 1.2414\n",
      "\t\tTraining Accuracy: 48.86%\t Validation Accuracy: 48.03%\n",
      "Epoch: 6\t100.00% complete. 2.59 seconds elapsed in epoch.\n",
      "Epoch: 6 \tTraining Loss: 1.2367 \tValidation Loss: 1.2387\n",
      "\t\tTraining Accuracy: 49.91%\t Validation Accuracy: 48.57%\n",
      "Epoch: 7\t100.00% complete. 2.57 seconds elapsed in epoch.\n",
      "Epoch: 7 \tTraining Loss: 1.2357 \tValidation Loss: 1.2379\n",
      "\t\tTraining Accuracy: 50.12%\t Validation Accuracy: 48.75%\n",
      "Epoch: 8\t100.00% complete. 2.63 seconds elapsed in epoch.\n",
      "Epoch: 8 \tTraining Loss: 1.2328 \tValidation Loss: 1.2357\n",
      "\t\tTraining Accuracy: 50.36%\t Validation Accuracy: 49.64%\n",
      "Epoch: 9\t100.00% complete. 2.62 seconds elapsed in epoch.\n",
      "Epoch: 9 \tTraining Loss: 1.2313 \tValidation Loss: 1.2313\n",
      "\t\tTraining Accuracy: 50.57%\t Validation Accuracy: 50.18%\n",
      "Epoch: 10\t100.00% complete. 2.68 seconds elapsed in epoch.\n",
      "Epoch: 10 \tTraining Loss: 1.2302 \tValidation Loss: 1.2293\n",
      "\t\tTraining Accuracy: 50.28%\t Validation Accuracy: 50.90%\n",
      "Epoch: 11\t100.00% complete. 2.64 seconds elapsed in epoch.\n",
      "Epoch: 11 \tTraining Loss: 1.2275 \tValidation Loss: 1.2263\n",
      "\t\tTraining Accuracy: 50.89%\t Validation Accuracy: 51.34%\n",
      "Epoch: 12\t100.00% complete. 2.71 seconds elapsed in epoch.\n",
      "Epoch: 12 \tTraining Loss: 1.2261 \tValidation Loss: 1.2205\n",
      "\t\tTraining Accuracy: 50.85%\t Validation Accuracy: 51.25%\n",
      "Epoch: 13\t100.00% complete. 2.67 seconds elapsed in epoch.\n",
      "Epoch: 13 \tTraining Loss: 1.2167 \tValidation Loss: 1.2060\n",
      "\t\tTraining Accuracy: 51.20%\t Validation Accuracy: 53.32%\n",
      "Epoch: 14\t100.00% complete. 2.76 seconds elapsed in epoch.\n",
      "Epoch: 14 \tTraining Loss: 1.2075 \tValidation Loss: 1.1936\n",
      "\t\tTraining Accuracy: 52.97%\t Validation Accuracy: 55.73%\n",
      "Epoch: 15\t100.00% complete. 2.64 seconds elapsed in epoch.\n",
      "Epoch: 15 \tTraining Loss: 1.1988 \tValidation Loss: 1.1881\n",
      "\t\tTraining Accuracy: 54.29%\t Validation Accuracy: 56.54%\n",
      "Epoch: 16\t100.00% complete. 2.73 seconds elapsed in epoch.\n",
      "Epoch: 16 \tTraining Loss: 1.1943 \tValidation Loss: 1.1830\n",
      "\t\tTraining Accuracy: 54.72%\t Validation Accuracy: 57.80%\n",
      "Epoch: 17\t100.00% complete. 2.88 seconds elapsed in epoch.\n",
      "Epoch: 17 \tTraining Loss: 1.1893 \tValidation Loss: 1.1833\n",
      "\t\tTraining Accuracy: 55.89%\t Validation Accuracy: 56.81%\n",
      "Epoch: 18\t100.00% complete. 2.75 seconds elapsed in epoch.\n",
      "Epoch: 18 \tTraining Loss: 1.1844 \tValidation Loss: 1.1780\n",
      "\t\tTraining Accuracy: 56.16%\t Validation Accuracy: 57.62%\n",
      "Epoch: 19\t100.00% complete. 2.69 seconds elapsed in epoch.\n",
      "Epoch: 19 \tTraining Loss: 1.1866 \tValidation Loss: 1.1767\n",
      "\t\tTraining Accuracy: 55.89%\t Validation Accuracy: 56.99%\n",
      "Epoch: 20\t100.00% complete. 2.80 seconds elapsed in epoch.\n",
      "Epoch: 20 \tTraining Loss: 1.1798 \tValidation Loss: 1.1742\n",
      "\t\tTraining Accuracy: 56.66%\t Validation Accuracy: 57.44%\n",
      "Epoch: 21\t100.00% complete. 2.77 seconds elapsed in epoch.\n",
      "Epoch: 21 \tTraining Loss: 1.1818 \tValidation Loss: 1.1671\n",
      "\t\tTraining Accuracy: 56.35%\t Validation Accuracy: 59.05%\n",
      "Epoch: 22\t100.00% complete. 2.72 seconds elapsed in epoch.\n",
      "Epoch: 22 \tTraining Loss: 1.1781 \tValidation Loss: 1.1652\n",
      "\t\tTraining Accuracy: 57.04%\t Validation Accuracy: 58.78%\n",
      "Epoch: 23\t100.00% complete. 2.72 seconds elapsed in epoch.\n",
      "Epoch: 23 \tTraining Loss: 1.1762 \tValidation Loss: 1.1623\n",
      "\t\tTraining Accuracy: 57.62%\t Validation Accuracy: 58.96%\n",
      "Epoch: 24\t100.00% complete. 2.72 seconds elapsed in epoch.\n",
      "Epoch: 24 \tTraining Loss: 1.1756 \tValidation Loss: 1.1604\n",
      "\t\tTraining Accuracy: 56.46%\t Validation Accuracy: 59.50%\n",
      "Epoch: 25\t100.00% complete. 2.71 seconds elapsed in epoch.\n",
      "Epoch: 25 \tTraining Loss: 1.1751 \tValidation Loss: 1.1604\n",
      "\t\tTraining Accuracy: 56.41%\t Validation Accuracy: 59.50%\n",
      "Epoch: 26\t100.00% complete. 2.75 seconds elapsed in epoch.\n",
      "Epoch: 26 \tTraining Loss: 1.1724 \tValidation Loss: 1.1586\n",
      "\t\tTraining Accuracy: 57.14%\t Validation Accuracy: 59.14%\n",
      "Epoch: 27\t100.00% complete. 2.74 seconds elapsed in epoch.\n",
      "Epoch: 27 \tTraining Loss: 1.1710 \tValidation Loss: 1.1586\n",
      "\t\tTraining Accuracy: 57.60%\t Validation Accuracy: 59.77%\n",
      "Epoch: 28\t100.00% complete. 2.74 seconds elapsed in epoch.\n",
      "Epoch: 28 \tTraining Loss: 1.1740 \tValidation Loss: 1.1590\n",
      "\t\tTraining Accuracy: 56.41%\t Validation Accuracy: 59.86%\n",
      "Epoch: 29\t100.00% complete. 2.77 seconds elapsed in epoch.\n",
      "Epoch: 29 \tTraining Loss: 1.1671 \tValidation Loss: 1.1588\n",
      "\t\tTraining Accuracy: 57.75%\t Validation Accuracy: 59.50%\n",
      "Epoch: 30\t100.00% complete. 2.75 seconds elapsed in epoch.\n",
      "Epoch: 30 \tTraining Loss: 1.1673 \tValidation Loss: 1.1629\n",
      "\t\tTraining Accuracy: 57.37%\t Validation Accuracy: 57.80%\n",
      "Epoch: 31\t100.00% complete. 2.77 seconds elapsed in epoch.\n",
      "Epoch: 31 \tTraining Loss: 1.1680 \tValidation Loss: 1.1575\n",
      "\t\tTraining Accuracy: 57.16%\t Validation Accuracy: 59.50%\n",
      "Epoch: 32\t100.00% complete. 2.81 seconds elapsed in epoch.\n",
      "Epoch: 32 \tTraining Loss: 1.1685 \tValidation Loss: 1.1534\n",
      "\t\tTraining Accuracy: 56.95%\t Validation Accuracy: 60.13%\n",
      "Epoch: 33\t100.00% complete. 2.68 seconds elapsed in epoch.\n",
      "Epoch: 33 \tTraining Loss: 1.1651 \tValidation Loss: 1.1570\n",
      "\t\tTraining Accuracy: 57.25%\t Validation Accuracy: 59.95%\n",
      "Epoch: 34\t100.00% complete. 2.79 seconds elapsed in epoch.\n",
      "Epoch: 34 \tTraining Loss: 1.1652 \tValidation Loss: 1.1586\n",
      "\t\tTraining Accuracy: 57.91%\t Validation Accuracy: 58.33%\n",
      "Epoch: 35\t100.00% complete. 2.77 seconds elapsed in epoch.\n",
      "Epoch: 35 \tTraining Loss: 1.1679 \tValidation Loss: 1.1562\n",
      "\t\tTraining Accuracy: 56.79%\t Validation Accuracy: 58.69%\n",
      "Epoch: 36\t100.00% complete. 2.70 seconds elapsed in epoch.\n",
      "Epoch: 36 \tTraining Loss: 1.1657 \tValidation Loss: 1.1505\n",
      "\t\tTraining Accuracy: 57.41%\t Validation Accuracy: 60.39%\n",
      "Epoch: 37\t100.00% complete. 2.69 seconds elapsed in epoch.\n",
      "Epoch: 37 \tTraining Loss: 1.1653 \tValidation Loss: 1.1579\n",
      "\t\tTraining Accuracy: 57.21%\t Validation Accuracy: 57.89%\n",
      "Epoch: 38\t100.00% complete. 2.69 seconds elapsed in epoch.\n",
      "Epoch: 38 \tTraining Loss: 1.1665 \tValidation Loss: 1.1539\n",
      "\t\tTraining Accuracy: 57.10%\t Validation Accuracy: 59.41%\n",
      "Epoch: 39\t100.00% complete. 2.76 seconds elapsed in epoch.\n",
      "Epoch: 39 \tTraining Loss: 1.1643 \tValidation Loss: 1.1466\n",
      "\t\tTraining Accuracy: 57.41%\t Validation Accuracy: 60.39%\n",
      "Epoch: 40\t100.00% complete. 2.73 seconds elapsed in epoch.\n",
      "Epoch: 40 \tTraining Loss: 1.1613 \tValidation Loss: 1.1467\n",
      "\t\tTraining Accuracy: 57.87%\t Validation Accuracy: 60.48%\n",
      "Epoch: 41\t100.00% complete. 2.74 seconds elapsed in epoch.\n",
      "Epoch: 41 \tTraining Loss: 1.1615 \tValidation Loss: 1.1449\n",
      "\t\tTraining Accuracy: 58.10%\t Validation Accuracy: 61.02%\n",
      "Epoch: 42\t100.00% complete. 2.77 seconds elapsed in epoch.\n",
      "Epoch: 42 \tTraining Loss: 1.1612 \tValidation Loss: 1.1473\n",
      "\t\tTraining Accuracy: 57.87%\t Validation Accuracy: 60.04%\n",
      "Epoch: 43\t100.00% complete. 2.74 seconds elapsed in epoch.\n",
      "Epoch: 43 \tTraining Loss: 1.1624 \tValidation Loss: 1.1474\n",
      "\t\tTraining Accuracy: 57.58%\t Validation Accuracy: 60.48%\n",
      "Epoch: 44\t100.00% complete. 2.77 seconds elapsed in epoch.\n",
      "Epoch: 44 \tTraining Loss: 1.1633 \tValidation Loss: 1.1458\n",
      "\t\tTraining Accuracy: 57.18%\t Validation Accuracy: 60.93%\n",
      "Epoch: 45\t100.00% complete. 2.74 seconds elapsed in epoch.\n",
      "Epoch: 45 \tTraining Loss: 1.1572 \tValidation Loss: 1.1466\n",
      "\t\tTraining Accuracy: 58.21%\t Validation Accuracy: 60.66%\n",
      "Epoch: 46\t100.00% complete. 2.76 seconds elapsed in epoch.\n",
      "Epoch: 46 \tTraining Loss: 1.1604 \tValidation Loss: 1.1452\n",
      "\t\tTraining Accuracy: 58.04%\t Validation Accuracy: 60.39%\n",
      "Epoch: 47\t100.00% complete. 2.75 seconds elapsed in epoch.\n",
      "Epoch: 47 \tTraining Loss: 1.1569 \tValidation Loss: 1.1452\n",
      "\t\tTraining Accuracy: 58.71%\t Validation Accuracy: 60.39%\n",
      "Epoch: 48\t100.00% complete. 2.78 seconds elapsed in epoch.\n",
      "Epoch: 48 \tTraining Loss: 1.1598 \tValidation Loss: 1.1463\n",
      "\t\tTraining Accuracy: 57.71%\t Validation Accuracy: 60.13%\n",
      "Epoch: 49\t100.00% complete. 2.70 seconds elapsed in epoch.\n",
      "Epoch: 49 \tTraining Loss: 1.1586 \tValidation Loss: 1.1411\n",
      "\t\tTraining Accuracy: 57.81%\t Validation Accuracy: 60.84%\n",
      "163.55 total seconds elapsed. 3.34 seconds per epoch.\n"
     ]
    }
   ],
   "source": [
    "model, history = train(model, train_dataloader, val_dataloader, test_dataloader, hyperpameters['epochs'], hyperpameters['optimizer'], criterion, train_on_gpu, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'Models/BERT_mini_whole.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('Models/BERT_mini_whole.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.6343e-01, 3.5388e-02, 1.1803e-03, 5.1527e-06], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "text = \"6 month ago today I lose my mom she die after a five year long battle with leukemia which you think would give I time to prepare but after four year in remission three week in the hospital and then her death I was just not prepared for this it defiantly do not feel like that long ago that I see she I still cry every day is that normal I miss her alot she was my good friend most people think I m fine because I work really hard at act normal but it is so hard I just miss she so much\"\n",
    "tokenized = tokenizer.encode_plus(text, add_special_tokens=False, max_length=200, padding='longest', return_tensors='pt', return_attention_mask=False, return_token_type_ids=False)\n",
    "tokenized['input_ids'] = tokenized['input_ids'].to(device)\n",
    "output = model(tokenized['input_ids'])\n",
    "print(output.logits[0])\n",
    "\n",
    "prediction = torch.argmax(output.logits[0]).item()\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prediction = torch.argmax(output.logits).item()\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
